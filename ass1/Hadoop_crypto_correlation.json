{"paragraphs":[{"text":"// import apache csv\nimport org.apache.spark.sql.types.{\n  StructType,\n  StructField,\n  StringType,\n  IntegerType,\n  DateType\n};\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.expressions.Window\n\nimport org.apache.spark.{SparkConf, SparkContext}\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.sql.functions.{col, month}\n\nimport org.apache.spark.SparkConf\nimport org.apache.spark.SparkContext\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.sql.functions.corr\n\n// import for youtube reacion\nimport org.apache.spark.{SparkConf, SparkContext}\nimport com.google.api.client.googleapis.javanet.GoogleNetHttpTransport // TODO check package for spark 1.x\nimport com.google.api.client.http.HttpTransport\nimport com.google.api.client.json.JsonFactory\nimport com.google.api.client.json.jackson2.JacksonFactory\nimport com.google.api.services.youtube.YouTube\nimport com.google.api.services.youtube.model.{SearchListResponse, VideoListResponse}\n\nimport scala.collection.JavaConverters._\nimport scala.util.{Failure, Success, Try}\n","dateUpdated":"2023-05-14T20:08:50+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","lineNumbers":true,"editorHide":false},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1683740731183_-1919510371","id":"20230510-174531_2020321027","result":{"code":"ERROR","type":"TEXT","msg":"import org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType, DateType}\nimport org.apache.spark.sql.Row\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.expressions.Window\nimport org.apache.spark.{SparkConf, SparkContext}\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.sql.functions.{col, month}\nimport org.apache.spark.{SparkConf, SparkContext}\n<console>:71: error: object api is not a member of package com.google\n         import com.google.api.client.googleapis.javanet.GoogleNetHttpTransport // TODO check package for spark 1.x\n                           ^\n"},"dateCreated":"2023-05-10T05:45:31+0000","dateStarted":"2023-05-14T19:57:39+0000","dateFinished":"2023-05-14T19:57:43+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:200","focus":true},{"text":"// get reaction from youtube channel\n// TODO check google package and member\nval jsonFactory: JsonFactory = JacksonFactory.getDefaultInstance\nval httpTransport: HttpTransport = GoogleNetHttpTransport.newTrustedTransport\nval apiKey: String = \"AIzaSyCB4BKyZ-_pb8ObGemr0Tsnqp_Xtvl5pC8\"\nval youtube: YouTube = new YouTube.Builder(httpTransport, jsonFactory, null)\n    .setApplicationName(\"YouTubeReactionsCSV\")\n    .build()\n\ndef main(args: Array[String]): Unit = {\nval sparkConf = new SparkConf().setAppName(\"YouTubeReactionsCSV\")\nval sc = new SparkContext(sparkConf)\n\n// Retrieve reactions on a specific channel's videos, \nval channelID = \"UCuifm5ns5SRG8LZJ6gCfKyw\"\nval searchResponse: SearchListResponse = youtube.search().list(\"id\")\n    .setChannelId(channelID)\n    .setType(\"video\")\n    .setMaxResults(50L) // 50 video's\n    .execute()\n\nval videoIds: List[String] = searchResponse.getItems.asScala.map(_.getId.getVideoId).toList\n\nval reactionsRDD = sc.parallelize(videoIds).flatMap(fetchVideoReactions)\nval reactions = reactionsRDD.collect().toList\n\n// Save reactions to a CSV file\nval csvFilename = \"C:/Development/hadoop/les1/youtube.csv\"\nval csvColumns = List(\"Video ID\", \"Likes\", \"Dislikes\", \"Comments\")\n\nval csvData = csvColumns.mkString(\",\") + \"\\n\" + reactions.map(reaction => csvColumns.map(reaction.getOrElse(_, \"\")).mkString(\",\")).mkString(\"\\n\")\n\nsc.parallelize(Seq(csvData)).saveAsTextFile(csvFilename)\n\nTry {\n    val response: VideoListResponse = youtube.videos().list(\"statistics\")\n        .setId(videoId)\n        .execute()\n    \n    val stats = response.getItems.get(0).getStatistics\n    val likes = stats.getLikeCount.toLong\n    val dislikes = stats.getDislikeCount.toLong\n    val comments = stats.getCommentCount.toLong\n    \n    Map(\"Video ID\" -> videoId, \"Likes\" -> likes, \"Dislikes\" -> dislikes, \"Comments\" -> comments)\n} match {\n    case Success(reaction) => Some(reaction)\n    case Failure(ex) => None\n}","dateUpdated":"2023-05-14T20:11:54+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1683881426228_352757071","id":"20230512-085026_2167770","result":{"code":"ERROR","type":"TEXT","msg":"<console>:73: error: not found: type JsonFactory\n       val jsonFactory: JsonFactory = JacksonFactory.getDefaultInstance\n                        ^\n<console>:73: error: not found: value JacksonFactory\n       val jsonFactory: JsonFactory = JacksonFactory.getDefaultInstance\n                                      ^\n"},"dateCreated":"2023-05-12T08:50:26+0000","dateStarted":"2023-05-14T19:57:39+0000","dateFinished":"2023-05-14T19:57:44+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:201","focus":true},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1684067059061_-1998832436","id":"20230514-122419_630269624","dateCreated":"2023-05-14T12:24:19+0000","status":"ERROR","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:536","dateUpdated":"2023-05-14T20:16:37+0000","dateFinished":"2023-05-14T19:57:48+0000","dateStarted":"2023-05-14T19:57:44+0000","result":{"code":"ERROR","type":"TEXT","msg":"conf: org.apache.spark.SparkConf = org.apache.spark.SparkConf@1888aee\norg.apache.spark.SparkException: Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:\norg.apache.spark.SparkContext.<init>(SparkContext.scala:82)\norg.apache.zeppelin.spark.SparkInterpreter.createSparkContext(SparkInterpreter.java:371)\norg.apache.zeppelin.spark.SparkInterpreter.getSparkContext(SparkInterpreter.java:138)\norg.apache.zeppelin.spark.SparkInterpreter.open(SparkInterpreter.java:598)\norg.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:69)\norg.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:93)\norg.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:341)\norg.apache.zeppelin.scheduler.Job.run(Job.java:176)\norg.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:139)\njava.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\njava.util.concurrent.FutureTask.run(FutureTask.java:266)\njava.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\njava.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\njava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\njava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\njava.lang.Thread.run(Thread.java:745)\n\tat org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$1.apply(SparkContext.scala:2281)\n\tat org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$1.apply(SparkContext.scala:2263)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2263)\n\tat org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2336)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:91)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:73)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:78)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:80)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:82)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:84)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:86)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:88)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:90)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:92)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:94)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:96)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:98)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:100)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:102)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:104)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:106)\n\tat $iwC$$iwC$$iwC$$iwC.<init>(<console>:108)\n\tat $iwC$$iwC$$iwC.<init>(<console>:110)\n\tat $iwC$$iwC.<init>(<console>:112)\n\tat $iwC.<init>(<console>:114)\n\tat <init>(<console>:116)\n\tat .<init>(<console>:120)\n\tat .<clinit>(<console>)\n\tat .<init>(<console>:7)\n\tat .<clinit>(<console>)\n\tat $print(<console>)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n\tat sun.reflect.GeneratedMethodAccessor53.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.zeppelin.spark.Utils.invokeMethod(Utils.java:38)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:717)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpretInput(SparkInterpreter.java:928)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:871)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:864)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:94)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:341)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:176)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:139)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\n"},"text":"val conf = new SparkConf()\n  .setAppName(\"CSV Reader\")\n  .setMaster(\"local\") // Set the master URL based on your deployment\n\nval sc = new SparkContext(conf)\nval sqlContext = new SQLContext(sc)\n\n\nval bitcoin_data = \"C:/Development/hadoop/les1/bitcoin_data_update.csv\"\n\nval df = sqlContext.read\n  .format(\"csv\")\n  .option(\"header\", \"true\") // If the CSV file has a header row\n  .option(\"inferSchema\", \"true\") // Infer the schema automatically\n  .load(bitcoin_data)\n\n// bitcoin_df.printSchema()\n\n// bitcoin_df.show()\n\nval dfWithMonthNumber = bitcoin_df.withColumn(\"monthNumber\", month(col(\"Date\")))\n\n// dfithMonthNumber.show()\n\n// clean closing per date\nval to_date_pattern = \"MM/dd/yyyy\"\nval bitcoin_final_df = bitcoin_df\n  .withColumn(\"Date\", to_date($\"Date\", to_date_pattern))\n  .withColumn(\"Year\", year(col(\"Date\")))\n  .withColumn(\"Month\", month(col(\"Date\")))\n  .withColumn(\"Day_of_Week\", dayofweek(col(\"Date\")))\n  .withColumn(\"pct_change\", ((col(\"Close\") - lag(\"Close\", 1)\n      .over(Window.partitionBy().orderBy(\"SNo\"))) / col(\"Close\")) * 100.0)\n  .drop(\"SNo\")\n\n// get data from youtube reaction\nval bitcoin_data = \"C:/Development/hadoop/les1/animals_comments.csv\"\n\nval df = sqlContext.read\n  .format(\"csv\")\n  .option(\"header\", \"true\")\n  .option(\"inferSchema\", \"true\")\n  .load(bitcoin_data)\n\n// Read the CSV file into a DataFrame\nval youtube_df = spark.read.csv(\"C:/Development/hadoop/les1/reactions.csv\",inferSchema=True,header=True)\nval reaction_df = sqlContext.read\n  .format(\"csv\")\n  .option(\"header\", \"true\")\n  .option(\"inferSchema\", \"true\")\n  .load(df_reactions)\n\n// clean up data frame\nval clean_youtube_DF: DataFrame = reaction_df\n  .na.fill(0, Seq(\"Likes\", \"Dislikes\", \"Comments\"))\n  .withColumn(\"Likes\", col(\"Likes\").cast(\"long\"))\n  .withColumn(\"Dislikes\", col(\"Dislikes\").cast(\"long\"))\n  .withColumn(\"Comments\", col(\"Comments\").cast(\"long\"))\n\n// clean_youtube_DF.show()\n\n// \nval sc = new SparkContext(conf)\nval sqlContext = new SQLContext(sc)\n\n"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1684094259554_-1352491437","id":"20230514-195739_1069506609","dateCreated":"2023-05-14T19:57:39+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:792"}],"name":"Hadoop_crypto_correlation","id":"2HZFZ1RC8","angularObjects":{"2BYG9GAN3:shared_process":[],"2C139U1BG:shared_process":[],"2BYWWZJPK:shared_process":[],"2C2VAKPN2:shared_process":[],"2BZC2RPUW:shared_process":[],"2BZQEH9EH:shared_process":[]},"config":{"looknfeel":"default"},"info":{}}